{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Deep Learning Workshop\n",
    "\n",
    "AI booster - May 27, 2020\n",
    "\n",
    "Viktor Kovryzhkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you run on tensorlfow 2.2.0. This notebook won't work on 2.1.0 ;(\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random generators\n",
    "# don't suggest to do it in production \n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "\n",
    "#### First, let's prepare a code which generates sliding windows. \n",
    "We are going to use numpy index_tricks for that. Function  takes original data and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(arr, window_size):\n",
    "    \"\"\" Takes original data matrix with shape (N, D) and creates sliding windows of size window_size. \n",
    "    Result matrix has shape (M, W, D), where\n",
    "        - M is number of observations equal to (N - window_size + 1)\n",
    "        - W is window_size\n",
    "        - D - dimensinality of the original feature space\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray \n",
    "        Original 2D matrix\n",
    "    window_size : int\n",
    "        Size of the sliding window\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result : np.ndarray \n",
    "        Resulting 3D matrix where each observation is a sliding window. \n",
    "    \"\"\"\n",
    "    arr = arr.astype(np.float)\n",
    "    (stride,) = arr.strides\n",
    "    return np.lib.index_tricks.as_strided(\n",
    "        arr,\n",
    "        (arr.shape[0] - window_size + 1, window_size),\n",
    "        strides=[stride, stride],\n",
    "        writeable=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# other helper functions for plotting losses and predictions later\n",
    "def plot_loss(history): \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    loss = history['loss']\n",
    "    line = plt.plot(range(len(loss)), loss)\n",
    "    line[0].set_label('Training loss')\n",
    "    if 'val_loss' in history: \n",
    "        val_loss = history['val_loss']\n",
    "        line = plt.plot(range(len(val_loss)), val_loss)\n",
    "        line[0].set_label('Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def plot_predictions(x, y_true, y_pred):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x_indexes = np.arange(x.shape[0])\n",
    "    y_indexes = np.arange(x.shape[0], x.shape[0] + y_true.shape[0])\n",
    "    line1 = plt.plot(x_indexes, x)\n",
    "    line2 = plt.plot(y_indexes, y_true)\n",
    "    line3 = plt.plot(y_indexes, y_pred)\n",
    "    \n",
    "    line1[0].set_label('Historical data')\n",
    "    line2[0].set_label('Ground truth')\n",
    "    line3[0].set_label('Forecast')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "class MeanScaler(object):\n",
    "    def __init__(self, copy=True):\n",
    "        self.copy = copy\n",
    "        self.scale_ = None\n",
    "        \n",
    "    def fit(self, X): \n",
    "        self.scale_ = np.mean(np.abs(X), axis=1, keepdims=True) + 1\n",
    "        return self\n",
    "    \n",
    "    def _reshaped_scale(self, X_shape): \n",
    "        new_shape = (self.scale_.shape[0],) + (1,) * (len(X_shape) - 1)\n",
    "        return np.reshape(self.scale_, new_shape)\n",
    "    \n",
    "    def transform(self, X): \n",
    "        if self.copy: \n",
    "            X = X.copy()\n",
    "        \n",
    "        scale_ = self._reshaped_scale(X.shape)\n",
    "        X /= scale_ \n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, X): \n",
    "        if self.copy: \n",
    "            X = X.copy()\n",
    "        \n",
    "        scale_ = self._reshaped_scale(X.shape)\n",
    "        X *= scale_\n",
    "        return X \n",
    "    \n",
    "    def fit_transform(self, X): \n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some data and highlight several sliding windows it it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv')\n",
    "\n",
    "y = df['Passengers'].values\n",
    "window_size = 18\n",
    "y_windows = create_sliding_windows(y, window_size)\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(np.arange(y.shape[0]), y)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "for i, window_idx in enumerate([10, 15, 50, 120]): \n",
    "    p = patches.Rectangle((window_idx, np.min(y)), window_size, 600, linewidth=2, edgecolor=colors[i], linestyle='--', facecolor='none')\n",
    "    ax.add_patch(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we need to have some way to create train/validation partitions.\n",
    "Ideally we would use something line [sklearn.model_selection.TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which allows to create several backtest and do something very similar to traditional cross validation. \n",
    "But we are going to use single backtest here to keep things simple.\n",
    "\n",
    "\n",
    "_Note we don't do shuffling on this function cause it will break \"time awareness\" in dataset and we will end up learning on future to predict the past._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(X, y, validation_pct=0.1): \n",
    "    size = X.shape[0]\n",
    "    validation_rows = int(size * validation_pct) \n",
    "    \n",
    "    index = np.arange(size)\n",
    "    validation = index[-validation_rows:]\n",
    "    training = index[:-validation_rows] \n",
    "    \n",
    "    return X[training], y[training], X[validation], y[validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare simple baselines.\n",
    "\n",
    "We will use 2 very simple baselines: \n",
    "- **latest naive baseline**: outputs last known value (e.g. If we have data up until Sunday and want to predict for next week - each day gets Sunday's value as prediction)\n",
    "- **seasonal naive baseline**: outputs last known seasonal value (e.g. If we have data up until Sunday and want to predict for next week - future Monday get's last known Monday value as prediction, Tuesday - last known Tuesday, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_baseline(x, y):\n",
    "    \"\"\" Simple naive baseline predictions. \n",
    "    For each row in y predicts latest known value. \n",
    "    \"\"\"\n",
    "    latest = x[:, -1, :]\n",
    "    y_pred = np.tile(latest, (1, y.shape[1]))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def seasonal_baseline(x, y, period): \n",
    "    \"\"\" Seasonal naive baseline predictions. \n",
    "    \"\"\"\n",
    "    indexes = np.arange(y.shape[1]) - period\n",
    "    return np.squeeze(x[:, indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare evaluation metrics.\n",
    "\n",
    "- MASE: mean absolute scaled error\n",
    "- sMAPE: scaled mean absolute percentage error\n",
    "- OWA: overall weighted average (M4 competition metric)\n",
    "- RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase(y_true, y_pred, y_naive_pred):\n",
    "    baseline_mae = mean_absolute_error(y_true, y_naive_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mae / baseline_mae\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    error = np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n",
    "    error[~np.isfinite(error)] = 0.0\n",
    "    return error.sum() * 100.0 / len(y_true)\n",
    "\n",
    "\n",
    "def owa(smape, smape_naive, mase, mase_naive): \n",
    "    return (smape / smape_naive + mase / mase_naive) / 2.\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred): \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, y_pred_naive): \n",
    "    RMSE = rmse(y_true, y_pred)\n",
    "    sMAPE = smape(y_true, y_pred)\n",
    "    sMAPE_naive = smape(y_true, y_pred_naive)\n",
    "    MASE = mase(y_true, y_pred, y_pred_naive)\n",
    "    MASE_naive = mase(y_true, y_pred_naive, y_pred_naive)\n",
    "    OWA = owa(sMAPE, sMAPE_naive, MASE, MASE_naive)\n",
    "    \n",
    "    print('\\tRMSE   {}'.format(RMSE))\n",
    "    print('\\tsMAPE  {}'.format(sMAPE))\n",
    "    print('\\tMASE   {}'.format(MASE))\n",
    "    print('\\tOWA    {}'.format(OWA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: evaluating baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['Passengers'].values\n",
    "fdw = 24\n",
    "fw = 12\n",
    "data_windows = create_sliding_windows(data, window_size=fdw + fw)\n",
    "X = data_windows[:, :fdw, np.newaxis]\n",
    "y = data_windows[:, fdw:]\n",
    "\n",
    "X_train, y_train, X_val, y_val = train_validation_split(X, y, validation_pct=0.2)\n",
    "\n",
    "y_pred_simple = simple_baseline(X_val, y_val)\n",
    "y_pred_seasonal = seasonal_baseline(X_val, y_val, period=12)\n",
    "\n",
    "print('Simple baseline')\n",
    "evaluate(y_val, y_pred_simple, y_pred_simple)\n",
    "\n",
    "print('Seasonal baseline')\n",
    "evaluate(y_val, y_pred_seasonal, y_pred_seasonal)\n",
    "\n",
    "plot_predictions(X_val[0], y_val[0], y_pred_simple[0])\n",
    "plot_predictions(X_val[0], y_val[0], y_pred_seasonal[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to build models! \n",
    "Let's start with very simple seq2seq-like network, which has RNN encoder, RNN decoder and linear output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cells(rnn_units):\n",
    "    cells = []\n",
    "    for units in rnn_units:\n",
    "        cells.append(keras.layers.LSTMCell(units))\n",
    "    return cells\n",
    "\n",
    "\n",
    "class LSTMEstimator(keras.Model): \n",
    "    def __init__(self, fw, rnn_units, output_activation='linear'): \n",
    "        super(LSTMEstimator, self).__init__()\n",
    "        \n",
    "        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n",
    "        self.repeat_layer = keras.layers.RepeatVector(fw)\n",
    "        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n",
    "        self.output_layer = keras.layers.Dense(1, activation=output_activation)\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        outputs = self.encoder_layer(inputs)\n",
    "        outputs = self.repeat_layer(outputs)\n",
    "        outputs = self.decoder_layer(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMEstimator(fw=12, rnn_units=[32])\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.Adam(0.01))\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val), \n",
    "    epochs=200, \n",
    "    batch_size=8,\n",
    "    shuffle=True, \n",
    "    verbose=False,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "i = 10\n",
    "plot_predictions(X_val[i], y_val[i], y_pred[i])\n",
    "evaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaler = MeanScaler()\n",
    "train_scaler.fit(X_train)\n",
    "\n",
    "val_scaler = MeanScaler()\n",
    "val_scaler.fit(X_val)\n",
    "\n",
    "model = LSTMEstimator(fw=12, rnn_units=[32])\n",
    "model.compile(loss='mae', optimizer=keras.optimizers.Adam(0.01))\n",
    "history = model.fit(\n",
    "    train_scaler.transform(X_train), \n",
    "    train_scaler.transform(y_train),\n",
    "    validation_data=(val_scaler.transform(X_val), val_scaler.transform(y_val)), \n",
    "    epochs=200, \n",
    "    batch_size=8,\n",
    "    shuffle=True, \n",
    "    verbose=False,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(val_scaler.transform(X_val))\n",
    "y_pred = val_scaler.inverse_transform(y_pred)\n",
    "y_pred_seasonal = seasonal_baseline(X_val, y_val, period=12)\n",
    "i = 10\n",
    "plot_predictions(X_val[i], y_val[i], y_pred[i])\n",
    "evaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GaussianLayer, self).__init__(**kwargs)\n",
    "        self.W_mu = None\n",
    "        self.b_mu = None\n",
    "        self.W_sigma = None\n",
    "        self.b_sigma = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(GaussianLayer, self).build(input_shape)\n",
    "        \n",
    "        dim = input_shape[-1]\n",
    "        \n",
    "        self.W_mu = self.add_weight(\n",
    "            name='W_mu', \n",
    "            shape=(dim, 1), \n",
    "            initializer='glorot_normal', \n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_mu = self.add_weight(\n",
    "            name='b_mu', \n",
    "            shape=(1,), \n",
    "            initializer='zeros', \n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        self.W_sigma = self.add_weight(\n",
    "            name='W_sigma',\n",
    "            shape=(dim, 1),\n",
    "            initializer='glorot_normal',\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_sigma = self.add_weight(\n",
    "            name='b_sigma',\n",
    "            shape=(1,),\n",
    "            initializer='zeros', \n",
    "            trainable=True,\n",
    "        )        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mu = K.dot(inputs, self.W_mu)\n",
    "        mu = K.bias_add(mu, self.b_mu, data_format='channels_last')\n",
    "        \n",
    "        sigma = K.dot(inputs, self.W_sigma)\n",
    "        sigma = K.bias_add(sigma, self.b_sigma, data_format='channels_last')\n",
    "        sigma = K.softplus(sigma) + K.epsilon()\n",
    "        \n",
    "        return tf.squeeze(mu, axis=-1), tf.squeeze(sigma, axis=-1) \n",
    "    \n",
    "\n",
    "def gaussian_loss(y_true, mu, sigma):\n",
    "    loss = (\n",
    "         tf.math.log(sigma) \n",
    "        + 0.5 * tf.math.log(2 * np.pi) \n",
    "        + 0.5 * tf.square(tf.math.divide(y_true - mu, sigma))\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "    \n",
    "def gaussian_sample(mu, sigma): \n",
    "    mu = tf.expand_dims(mu, axis=-1)\n",
    "    sigma = tf.expand_dims(sigma, axis=-1)\n",
    "    \n",
    "    samples = tf.random.normal((300,), mean=mu, stddev=sigma)\n",
    "    return tf.reduce_mean(samples, axis=-1)\n",
    "\n",
    "    \n",
    "class DeepAR(keras.Model): \n",
    "    def __init__(self, fw, rnn_units): \n",
    "        super(DeepAR, self).__init__()\n",
    "        \n",
    "        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n",
    "        self.repeat_layer = keras.layers.RepeatVector(fw)\n",
    "        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n",
    "        self.gaussian_layer = GaussianLayer()\n",
    "\n",
    "    def call(self, inputs): \n",
    "        outputs = self.encoder_layer(inputs)\n",
    "        outputs = self.repeat_layer(outputs)\n",
    "        outputs = self.decoder_layer(outputs)\n",
    "        return self.gaussian_layer(outputs)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            mu, sigma = self(x)\n",
    "            loss_val = self.loss(y, mu, sigma)\n",
    "\n",
    "        grads = tape.gradient(loss_val, self.trainable_weights)\n",
    "        grads = [tf.clip_by_value(grad, -1., 1.) for grad in grads]\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return {'loss': loss_val}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        x, y = inputs\n",
    "        mu, sigma = self(x, training=False)\n",
    "        loss_val = self.loss(y, mu, sigma)\n",
    "        return {'loss': loss_val}\n",
    "    \n",
    "    def predict_step(self, inputs): \n",
    "        x, = inputs\n",
    "        mu, sigma = self(x, training=False)\n",
    "        return gaussian_sample(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_units = [32]\n",
    "\n",
    "deep_ar = DeepAR(fw, rnn_units)\n",
    "\n",
    "train_scaler = MeanScaler()\n",
    "train_scaler.fit(X_train)\n",
    "\n",
    "val_scaler = MeanScaler()\n",
    "val_scaler.fit(X_val)\n",
    "\n",
    "deep_ar.compile(loss=gaussian_loss, optimizer=keras.optimizers.Adam(0.01))\n",
    "\n",
    "history = deep_ar.fit(\n",
    "    train_scaler.transform(X_train), \n",
    "    train_scaler.transform(y_train),\n",
    "    validation_data=(val_scaler.transform(X_val), val_scaler.transform(y_val)), \n",
    "    epochs=200, \n",
    "    batch_size=8,\n",
    "    shuffle=True, \n",
    "    verbose=False,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = deep_ar.predict(val_scaler.transform(X_val))\n",
    "y_pred = val_scaler.inverse_transform(y_pred)\n",
    "i = 10\n",
    "plot_predictions(X_val[i], y_val[i], y_pred[i])\n",
    "evaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling Poisson distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PoissonLayer, self).__init__(**kwargs)\n",
    "        self.W_rate = None\n",
    "        self.b_rate = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PoissonLayer, self).build(input_shape)\n",
    "        \n",
    "        dim = input_shape[0][-1]\n",
    "        \n",
    "        self.W_rate = self.add_weight(\n",
    "            name='W_rate', \n",
    "            shape=(dim, 1), \n",
    "            initializer='glorot_uniform', \n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b_rate = self.add_weight(\n",
    "            name='b_rate', \n",
    "            shape=(1,), \n",
    "            initializer='zeros', \n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, scale = inputs\n",
    "        \n",
    "        rate = K.dot(x, self.W_rate)\n",
    "        rate = K.bias_add(rate, self.b_rate, data_format='channels_last')\n",
    "        rate = K.softplus(rate) + K.epsilon()\n",
    "        rate = rate * scale\n",
    "        \n",
    "        return tf.squeeze(rate, axis=-1)\n",
    "\n",
    "\n",
    "class MeanScalerLayer(keras.layers.Layer): \n",
    "    \n",
    "    def call(self, inputs): \n",
    "        scale = tf.reduce_mean(tf.abs(inputs), axis=1, keepdims=True) + 1.0\n",
    "        outputs = inputs / scale\n",
    "        return outputs, scale\n",
    "\n",
    "\n",
    "def poisson_loss(y_true, rate): \n",
    "    loss = -1.0 * (\n",
    "        y_true * tf.math.log(rate) \n",
    "        - tf.math.lgamma(y_true + 1.0) \n",
    "        - rate\n",
    "    )\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def poisson_sample(rate): \n",
    "    sample = tf.random.poisson((300,), lam=rate)\n",
    "    sample = tf.reduce_mean(sample, axis=0)\n",
    "    return tf.expand_dims(sample, axis=-1)\n",
    "\n",
    "\n",
    "class DeepARPos(keras.Model): \n",
    "    def __init__(self, fw, rnn_units): \n",
    "        super(DeepARPos, self).__init__()\n",
    "        \n",
    "        self.mean_scaler_layer = MeanScalerLayer()\n",
    "        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n",
    "        self.repeat_layer = keras.layers.RepeatVector(fw)\n",
    "        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n",
    "        self.poisson_layer = PoissonLayer()\n",
    "\n",
    "    def call(self, inputs): \n",
    "        outputs, scale = self.mean_scaler_layer(inputs)\n",
    "        outputs = self.encoder_layer(outputs)\n",
    "        outputs = self.repeat_layer(outputs)\n",
    "        outputs = self.decoder_layer(outputs)\n",
    "        return self.poisson_layer([outputs, scale])\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        x, y = inputs\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            rate = self(x)\n",
    "            loss_val = self.loss(y, rate)\n",
    "    \n",
    "        grads = tape.gradient(loss_val, self.trainable_weights)\n",
    "        grads = [tf.clip_by_value(grad, -1., 1.) for grad in grads]\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        return {'loss': loss_val}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        x, y = inputs\n",
    "        rate = self(x, training=False)\n",
    "        loss_val = self.loss(y, rate)\n",
    "        return {'loss': loss_val}\n",
    "    \n",
    "    def predict_step(self, inputs): \n",
    "        x, = inputs\n",
    "        rate = self(x, training=False)\n",
    "        return poisson_sample(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_units = [32]\n",
    "\n",
    "deep_ar = DeepARPos(fw, rnn_units)\n",
    "deep_ar.compile(loss=poisson_loss, optimizer=keras.optimizers.Adam(0.01))\n",
    "\n",
    "history = deep_ar.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val), \n",
    "    epochs=200, \n",
    "    batch_size=8,\n",
    "    shuffle=True, \n",
    "    verbose=False,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = deep_ar.predict(X_val)\n",
    "\n",
    "i = 10\n",
    "plot_predictions(X_val[i], y_val[i], y_pred[i])\n",
    "evaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYC hourly energy consumption dataset (if there is a time)\n",
    "\n",
    "Try same with nyc_energy dataset, whih exposes multiple seasonality (24h and 1 week).  \n",
    "Dataset is big so feel free to sample training data for faster experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/nyc_energy.csv')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df.index[:500], df['demand'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['demand'].values\n",
    "data = data[~np.isnan(data)]\n",
    "fdw = 168\n",
    "fw = 24\n",
    "data_windows = create_sliding_windows(data, window_size=fdw + fw)\n",
    "X = data_windows[:, :fdw, np.newaxis]\n",
    "y = data_windows[:, fdw:]\n",
    "\n",
    "X_train, y_train, X_val, y_val = train_validation_split(X, y, validation_pct=0.2)\n",
    "\n",
    "y_pred_simple = simple_baseline(X_val, y_val)\n",
    "y_pred_seasonal = seasonal_baseline(X_val, y_val, period=24)\n",
    "\n",
    "print('Simple baseline')\n",
    "evaluate(y_val, y_pred_simple, y_pred_simple)\n",
    "\n",
    "print('Seasonal baseline')\n",
    "evaluate(y_val, y_pred_seasonal, y_pred_seasonal)\n",
    "\n",
    "plot_predictions(X_val[0], y_val[0], y_pred_simple[0])\n",
    "plot_predictions(X_val[0], y_val[0], y_pred_seasonal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsdlworkshop",
   "language": "python",
   "name": "tsdlworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
